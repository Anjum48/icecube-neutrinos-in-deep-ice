{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2n stage training\n",
    "\n",
    "* In the 2nd stage, the training data is divided into two parts, easy and hard, based on the prediction results of the 1st stage.\n",
    "* finetuning from 1st stage training\n",
    "* part A training easy part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb_api = \"xxxxx\" #user_secrets.get_secret(\"wandb_api\")\n",
    "wandb.login(key=wandb_api)\n",
    "    \n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "NAME='split02-1'\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    project=\"ICECUBE\",\n",
    "    entity=\"yamsam\",\n",
    "    name=NAME,\n",
    "    group='graphnet-tune',\n",
    "    log_model='all',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "train_db = '../../../input/batch_301-350.db'\n",
    "valid_db = '../../../input/dynedge-pretrained/batch_51.db'\n",
    "\n",
    "error_file = '../split01/result_300-350.csv' # prediction from 1st model include direction kappa\n",
    "retrain_dict = '../split01/last_state_dict_split01_first.pth'\n",
    "first_half=True\n",
    "\n",
    "# Append to PATH\n",
    "import sys\n",
    "sys.path.append('../../../input/graphnet/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from typing import Any, Dict, List, Optional\n",
    "import numpy as np\n",
    "\n",
    "from graphnet.data.sqlite.sqlite_utilities import create_table\n",
    "\n",
    "def load_input(meta_batch: pd.DataFrame, input_data_folder: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Will load the corresponding detector readings associated with the meta data batch.\n",
    "        \"\"\"\n",
    "        batch_id = pd.unique(meta_batch['batch_id'])\n",
    "\n",
    "        assert len(batch_id) == 1, \"contains multiple batch_ids. Did you set the batch_size correctly?\"\n",
    "        \n",
    "        detector_readings = pd.read_parquet(path = f'{input_data_folder}/batch_{batch_id[0]}.parquet')\n",
    "        sensor_positions = geometry_table.loc[detector_readings['sensor_id'], ['x', 'y', 'z']]\n",
    "        sensor_positions.index = detector_readings.index\n",
    "\n",
    "        for column in sensor_positions.columns:\n",
    "            if column not in detector_readings.columns:\n",
    "                detector_readings[column] = sensor_positions[column]\n",
    "\n",
    "        detector_readings['auxiliary'] = detector_readings['auxiliary'].replace({True: 1, False: 0})\n",
    "        return detector_readings.reset_index()\n",
    "\n",
    "def add_to_table(database_path: str,\n",
    "                      df: pd.DataFrame,\n",
    "                      table_name:  str,\n",
    "                      is_primary_key: bool,\n",
    "                      ) -> None:\n",
    "    \"\"\"Writes meta data to sqlite table. \n",
    "\n",
    "    Args:\n",
    "        database_path (str): the path to the database file.\n",
    "        df (pd.DataFrame): the dataframe that is being written to table.\n",
    "        table_name (str, optional): The name of the meta table. Defaults to 'meta_table'.\n",
    "        is_primary_key(bool): Must be True if each row of df corresponds to a unique event_id. Defaults to False.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        create_table(   columns=  df.columns,\n",
    "                        database_path = database_path, \n",
    "                        table_name = table_name,\n",
    "                        integer_primary_key= is_primary_key,\n",
    "                        index_column = 'event_id')\n",
    "    except sqlite3.OperationalError as e:\n",
    "        if 'already exists' in str(e):\n",
    "            pass\n",
    "        else:\n",
    "            raise e\n",
    "    engine = sqlalchemy.create_engine(\"sqlite:///\" + database_path)\n",
    "    df.to_sql(table_name, con=engine, index=False, if_exists=\"append\", chunksize = 200000)\n",
    "    engine.dispose()\n",
    "    return\n",
    "\n",
    "def convert_to_sqlite(meta_data_path: str,\n",
    "                      database_path: str,\n",
    "                      input_data_folder: str,\n",
    "                      batch_size: int = 200000,\n",
    "                      batch_ids: Optional[List[int]] = None,) -> None:\n",
    "    \"\"\"Converts a selection of the Competition's parquet files to a single sqlite database.\n",
    "\n",
    "    Args:\n",
    "        meta_data_path (str): Path to the meta data file.\n",
    "        batch_size (int): the number of rows extracted from meta data file at a time. Keep low for memory efficiency.\n",
    "        database_path (str): path to database. E.g. '/my_folder/data/my_new_database.db'\n",
    "        input_data_folder (str): folder containing the parquet input files.\n",
    "        batch_ids (List[int]): The batch_ids you want converted. Defaults to None (all batches will be converted)\n",
    "    \"\"\"\n",
    "    if batch_ids is None:\n",
    "        batch_ids = np.arange(1,661,1).to_list()\n",
    "    else:\n",
    "        assert isinstance(batch_ids,list), \"Variable 'batch_ids' must be list.\"\n",
    "    if not database_path.endswith('.db'):\n",
    "        database_path = database_path+'.db'\n",
    "    meta_data_iter = pq.ParquetFile(meta_data_path).iter_batches(batch_size = batch_size)\n",
    "    batch_id = 1\n",
    "    converted_batches = []\n",
    "    progress_bar = tqdm(total = len(batch_ids))\n",
    "    for meta_data_batch in meta_data_iter:\n",
    "        if batch_id in batch_ids:\n",
    "            meta_data_batch  = meta_data_batch.to_pandas()\n",
    "            add_to_table(database_path = database_path,\n",
    "                        df = meta_data_batch,\n",
    "                        table_name='meta_table',\n",
    "                        is_primary_key= True)\n",
    "            pulses = load_input(meta_batch=meta_data_batch, input_data_folder= input_data_folder)\n",
    "            del meta_data_batch # memory\n",
    "            add_to_table(database_path = database_path,\n",
    "                        df = pulses,\n",
    "                        table_name='pulse_table',\n",
    "                        is_primary_key= False)\n",
    "            del pulses # memory\n",
    "            progress_bar.update(1)\n",
    "            converted_batches.append(batch_id)\n",
    "        batch_id +=1\n",
    "        if len(batch_ids) == len(converted_batches):\n",
    "            break\n",
    "    progress_bar.close()\n",
    "    del meta_data_iter # memory\n",
    "    print(f'Conversion Complete!. Database available at\\n {database_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining A Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def make_selection(df: pd.DataFrame, pulse_threshold: int = 200) -> None:\n",
    "    \"\"\"Creates a validation and training selection (20 - 80). All events in both selections satisfies n_pulses <= 200 by default. \"\"\"\n",
    "    n_events = np.arange(0, len(df),1)\n",
    "    train_selection, validate_selection = train_test_split(n_events, \n",
    "                                                                    shuffle=True, \n",
    "                                                                    random_state = 42, \n",
    "                                                                    test_size=0.20) \n",
    "    df['train'] = 0\n",
    "    df['validate'] = 0\n",
    "    \n",
    "    df['train'][train_selection] = 1\n",
    "    df['validate'][validate_selection] = 1\n",
    "    \n",
    "    assert len(train_selection) == sum(df['train'])\n",
    "    assert len(validate_selection) == sum(df['validate'])\n",
    "    \n",
    "    for selection in ['train', 'validate']:\n",
    "        df.loc[df[selection] == 1, :].to_csv(f'{selection}_selection_max_{pulse_threshold}_pulses.csv')\n",
    "    return\n",
    "\n",
    "def get_number_of_pulses(db: str, event_id: int, pulsemap: str) -> int:\n",
    "    with sqlite3.connect(db) as con:\n",
    "        query = f'select event_id from {pulsemap} where event_id = {event_id} limit 20000'\n",
    "        data = con.execute(query).fetchall()\n",
    "    return len(data)\n",
    "\n",
    "def count_pulses(database: str, pulsemap: str) -> pd.DataFrame:\n",
    "    \"\"\" Will count the number of pulses in each event and return a single dataframe that contains counts for each event_id.\"\"\"\n",
    "    with sqlite3.connect(database) as con:\n",
    "        query = 'select event_id from meta_table'\n",
    "        events = pd.read_sql(query,con)\n",
    "    counts = {'event_id': [],\n",
    "              'n_pulses': []}\n",
    "    for event_id in tqdm(events['event_id']):\n",
    "        a = get_number_of_pulses(database, event_id, pulsemap)\n",
    "        counts['event_id'].append(event_id)\n",
    "        counts['n_pulses'].append(a)\n",
    "    df = pd.DataFrame(counts)\n",
    "    df.to_csv('counts.csv')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the 2nd stage, the training data is divided into two parts, easy and hard, based on the prediction results of the 1st stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_selection_half(df: pd.DataFrame, postfix) -> None:\n",
    "    \"\"\"Creates a validation and training selection (20 - 80). All events in both selections satisfies n_pulses <= 200 by default. \"\"\"\n",
    "    n_events = np.arange(0, len(df),1)\n",
    "    train_selection, validate_selection = train_test_split(n_events, \n",
    "                                                                    shuffle=True, \n",
    "                                                                    random_state = 42, \n",
    "                                                                    test_size=0.20) \n",
    "    df['train'] = 0\n",
    "    df['validate'] = 0\n",
    "    \n",
    "    df['train'][train_selection] = 1\n",
    "    df['validate'][validate_selection] = 1\n",
    "    \n",
    "    assert len(train_selection) == sum(df['train'])\n",
    "    assert len(validate_selection) == sum(df['validate'])\n",
    "\n",
    "    for selection in ['train', 'validate']:\n",
    "        df.loc[df[selection] == 1, :].to_csv(f'{selection}_selection_{postfix}_pulses.csv')\n",
    "    return\n",
    "\n",
    "pulsemap = 'pulse_table'\n",
    "database = train_db\n",
    "\n",
    "df = count_pulses(database, pulsemap)\n",
    "\n",
    "edf = pd.read_csv(error_file)\n",
    "edf['event_id']=edf.event_id.astype(int)\n",
    "edf['first_half'] = 1/np.sqrt(edf['direction_kappa']) <= 0.5\n",
    "df = df.merge(edf[['event_id', 'first_half']], on='event_id')\n",
    "df1 = df[df.first_half ==1].reset_index(drop=True)\n",
    "df2 = df[df.first_half ==0].reset_index(drop=True)\n",
    "print (df1.shape, df2.shape)\n",
    "\n",
    "make_selection_half(df1, 'first')\n",
    "make_selection_half(df2, 'second')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training DynEdge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "\n",
    "from torch.optim.adam import Adam\n",
    "from graphnet.data.constants import FEATURES, TRUTH\n",
    "from graphnet.models import StandardModel\n",
    "from graphnet.models.detector.icecube import IceCubeKaggle\n",
    "from graphnet.models.gnn import DynEdge\n",
    "from graphnet.models.graph_builders import KNNGraphBuilder\n",
    "from graphnet.models.task.reconstruction import DirectionReconstructionWithKappa, ZenithReconstructionWithKappa, AzimuthReconstructionWithKappa\n",
    "from graphnet.training.callbacks import ProgressBar, PiecewiseLinearLR\n",
    "from graphnet.training.loss_functions import VonMisesFisher3DLoss, VonMisesFisher2DLoss\n",
    "from graphnet.training.labels import Direction\n",
    "from graphnet.training.utils import make_dataloader\n",
    "from graphnet.utilities.logging import Logger\n",
    "from pytorch_lightning import Trainer\n",
    "import pandas as pd\n",
    "\n",
    "logger = Logger()\n",
    "\n",
    "def build_model(config: Dict[str,Any], train_dataloader: Any) -> StandardModel:\n",
    "    \"\"\"Builds GNN from config\"\"\"\n",
    "    # Building model\n",
    "    detector = IceCubeKaggle(\n",
    "        graph_builder=KNNGraphBuilder(nb_nearest_neighbours=8),\n",
    "    )\n",
    "    gnn = DynEdge(\n",
    "        nb_inputs=detector.nb_outputs,\n",
    "        global_pooling_schemes=[\"min\", \"max\", \"mean\"],\n",
    "    )\n",
    "\n",
    "    if config[\"target\"] == 'direction':\n",
    "        task = DirectionReconstructionWithKappa(\n",
    "            hidden_size=gnn.nb_outputs,\n",
    "            target_labels=config[\"target\"],\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "        prediction_columns = [config[\"target\"] + \"_x\", \n",
    "                              config[\"target\"] + \"_y\", \n",
    "                              config[\"target\"] + \"_z\", \n",
    "                              config[\"target\"] + \"_kappa\" ]\n",
    "        additional_attributes = ['zenith', 'azimuth', 'event_id']\n",
    "\n",
    "    model = StandardModel(\n",
    "        detector=detector,\n",
    "        gnn=gnn,\n",
    "        tasks=[task],\n",
    "        optimizer_class=Adam,        \n",
    "#       optimizer_kwargs={\"lr\": 1e-05, \"eps\": 1e-03},\n",
    "        optimizer_kwargs={\"lr\": 1e-03, \"eps\": 1e-03},\n",
    "#       optimizer_kwargs={\"lr\": 1e-06, \"eps\": 1e-03},\n",
    "        scheduler_class=PiecewiseLinearLR,\n",
    "        scheduler_kwargs={\n",
    "           \"milestones\": [\n",
    "               0,\n",
    "               len(train_dataloader) / 2,\n",
    "               len(train_dataloader) * config[\"fit\"][\"max_epochs\"],\n",
    "           ],\n",
    "           \"factors\": [1e-01, 1, 1e-02],\n",
    "       },\n",
    "        scheduler_config={\n",
    "            \"interval\": \"step\",\n",
    "        },\n",
    "    )\n",
    "    model.prediction_columns = prediction_columns\n",
    "    model.additional_attributes = additional_attributes\n",
    "    \n",
    "    return model\n",
    "\n",
    "def load_pretrained_model(config: Dict[str,Any], state_dict_path: str = '../../../input/dynedge-pretrained/dynedge_pretrained_batch_1_to_50/state_dict.pth') -> StandardModel:\n",
    "    train_dataloader, _ = make_dataloaders(config = config)\n",
    "    model = build_model(config = config, \n",
    "                        train_dataloader = train_dataloader)\n",
    "    #model._inference_trainer = Trainer(config['fit'])\n",
    "    model.load_state_dict(state_dict_path)\n",
    "    model.prediction_columns = [config[\"target\"] + \"_x\", \n",
    "                              config[\"target\"] + \"_y\", \n",
    "                              config[\"target\"] + \"_z\", \n",
    "                              config[\"target\"] + \"_kappa\" ]\n",
    "    model.additional_attributes = ['zenith', 'azimuth', 'event_id']\n",
    "    return model\n",
    "\n",
    "def make_dataloaders(config: Dict[str, Any]) -> List[Any]:\n",
    "    \"\"\"Constructs training and validation dataloaders for training with early stopping.\"\"\"\n",
    "    train_dataloader = make_dataloader(db = config['path'],\n",
    "                                            selection = pd.read_csv(config['train_selection'])[config['index_column']].ravel().tolist(),\n",
    "                                            pulsemaps = config['pulsemap'],\n",
    "                                            features = features,\n",
    "                                            truth = truth,\n",
    "                                            batch_size = config['batch_size'],\n",
    "                                            num_workers = config['num_workers'],\n",
    "                                            shuffle = False,\n",
    "                                            labels = {'direction': Direction()},\n",
    "                                            index_column = config['index_column'],\n",
    "                                            truth_table = config['truth_table'],\n",
    "                                            sample_limit = config['sample_limit']\n",
    "                                            )\n",
    "    \n",
    "    validate_dataloader = make_dataloader(db = config['path'],\n",
    "                                            selection = pd.read_csv(config['validate_selection'])[config['index_column']].ravel().tolist(),\n",
    "                                            pulsemaps = config['pulsemap'],\n",
    "                                            features = features,\n",
    "                                            truth = truth,\n",
    "                                            batch_size = config['batch_size'],\n",
    "                                            num_workers = config['num_workers'],\n",
    "                                            shuffle = False,\n",
    "                                            labels = {'direction': Direction()},\n",
    "                                            index_column = config['index_column'],\n",
    "                                            truth_table = config['truth_table'],\n",
    "                                            sample_limit = config['sample_limit']                                       \n",
    "                                            )\n",
    "    return train_dataloader, validate_dataloader\n",
    "\n",
    "def train_dynedge_from_scratch(config: Dict[str, Any]) -> StandardModel:\n",
    "    \"\"\"Builds and trains GNN according to config.\"\"\"\n",
    "    logger.info(f\"features: {config['features']}\")\n",
    "    logger.info(f\"truth: {config['truth']}\")\n",
    "    \n",
    "    archive = os.path.join(config['base_dir'], \"train_model_without_configs\")\n",
    "    run_name = f\"dynedge_{config['target']}_{config['run_name_tag']}\"\n",
    "\n",
    "    train_dataloader, validate_dataloader = make_dataloaders(config = config)\n",
    "\n",
    "    model = build_model(config, train_dataloader)\n",
    "   \n",
    "    loss_checkpoint = ModelCheckpoint(\n",
    "        dirpath='./',\n",
    "        filename=f\"best_loss_{NAME}\",\n",
    "        monitor=\"val_loss\",\n",
    "        save_last=True,\n",
    "        save_top_k=1,\n",
    "        save_weights_only=True,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "    \n",
    "    # Training model\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=config[\"early_stopping_patience\"],\n",
    "        ),\n",
    "        loss_checkpoint\n",
    "        #TQDMProgressBar(refresh_rate=10)\n",
    "#        ProgressBar(),\n",
    "    ]\n",
    "    \n",
    "    model.fit(\n",
    "        train_dataloader,\n",
    "        validate_dataloader,\n",
    "        callbacks=callbacks,\n",
    "        **config[\"fit\"],\n",
    "        logger=wandb_logger\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def inference(model, config: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"Applies model to the database specified in config['inference_database_path'] and saves results to disk.\"\"\"\n",
    "    # Make Dataloader\n",
    "    test_dataloader = make_dataloader(db = config['inference_database_path'],\n",
    "                                            selection = None, # Entire database\n",
    "                                            pulsemaps = config['pulsemap'],\n",
    "                                            features = features,\n",
    "                                            truth = truth,\n",
    "                                            batch_size = config['batch_size'],\n",
    "                                            num_workers = config['num_workers'],\n",
    "                                            shuffle = False,\n",
    "                                            labels = {'direction': Direction()},\n",
    "                                            index_column = config['index_column'],\n",
    "                                            truth_table = config['truth_table'],\n",
    "                                            )\n",
    "    \n",
    "    # Get predictions\n",
    "    results = model.predict_as_dataframe(\n",
    "        gpus = [0],\n",
    "        dataloader = test_dataloader,\n",
    "        prediction_columns=model.prediction_columns,\n",
    "        additional_attributes=model.additional_attributes,\n",
    "    )\n",
    "    # Save predictions and model to file\n",
    "    archive = os.path.join(config['base_dir'], \"train_model_without_configs\")\n",
    "    run_name = f\"dynedge_{config['target']}_{config['run_name_tag']}\"\n",
    "    db_name = config['path'].split(\"/\")[-1].split(\".\")[0]\n",
    "    path = os.path.join(archive, db_name, run_name)\n",
    "    logger.info(f\"Writing results to {path}\")\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    results.to_csv(f\"{path}/results.csv\")\n",
    "    return results\n",
    "\n",
    "class MyProgressBar(TQDMProgressBar):\n",
    "    def init_validation_tqdm(self):\n",
    "        bar = super().init_validation_tqdm()\n",
    "        if not sys.stdout.isatty():\n",
    "            bar.disable = True\n",
    "        return bar\n",
    "\n",
    "    def init_predict_tqdm(self):\n",
    "        bar = super().init_predict_tqdm()\n",
    "        if not sys.stdout.isatty():\n",
    "            bar.disable = True\n",
    "        return bar\n",
    "\n",
    "    def init_test_tqdm(self):\n",
    "        bar = super().init_test_tqdm()\n",
    "        if not sys.stdout.isatty():\n",
    "            bar.disable = True\n",
    "        return bar\n",
    "\n",
    "    \n",
    "def train_dynedge_restart(model, config: Dict[str, Any]) -> StandardModel:\n",
    "    \"\"\"Builds and trains GNN according to config.\"\"\"\n",
    "    logger.info(f\"features: {config['features']}\")\n",
    "    logger.info(f\"truth: {config['truth']}\")\n",
    "    \n",
    "    archive = os.path.join(config['base_dir'], \"train_model_without_configs\")\n",
    "    run_name = f\"dynedge_{config['target']}_{config['run_name_tag']}\"\n",
    "\n",
    "    train_dataloader, validate_dataloader = make_dataloaders(config = config)\n",
    "\n",
    "    #model = build_model(config, train_dataloader)\n",
    "    lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "    # Training model\n",
    "    \n",
    "    loss_checkpoint = ModelCheckpoint(\n",
    "        dirpath='./',\n",
    "        filename=f\"best_loss_{NAME}\",\n",
    "        monitor=\"val_loss\",\n",
    "        save_last=True,\n",
    "        save_top_k=1,\n",
    "        save_weights_only=True,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=config[\"early_stopping_patience\"],\n",
    "        ),\n",
    "        MyProgressBar(),\n",
    "        lr_monitor,\n",
    "        loss_checkpoint\n",
    "    ]\n",
    "\n",
    "    model.fit(\n",
    "        train_dataloader,\n",
    "        validate_dataloader,\n",
    "        callbacks=callbacks,\n",
    "        **config[\"fit\"],\n",
    "        logger=wandb_logger\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "features = FEATURES.KAGGLE\n",
    "truth = TRUTH.KAGGLE\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "        \"path\": train_db,\n",
    "        \"inference_database_path\": valid_db,\n",
    "        \"pulsemap\": 'pulse_table',\n",
    "        \"truth_table\": 'meta_table',\n",
    "        \"features\": features,\n",
    "        \"truth\": truth,\n",
    "        \"index_column\": 'event_id',\n",
    "        \"run_name_tag\": 'my_example',\n",
    "        \"batch_size\":512, # 512\n",
    "        \"sample_limit\":1000,\n",
    "        \"num_workers\": 8,\n",
    "        \"target\": 'direction',\n",
    "        \"early_stopping_patience\": 5,\n",
    "        \"fit\": {\n",
    "                \"max_epochs\": 20,\n",
    "                \"gpus\": [0],\n",
    "                \"distribution_strategy\": None,\n",
    "                #\"resume_from_checkpoint\": \"../exp10\"\n",
    "                #\"enable_progress_bar\":False\n",
    "                },\n",
    "#        'train_selection': 'train_selection_first_pulses.csv',\n",
    "#        'validate_selection': 'validate_selection_second_pulses.csv',\n",
    "        'test_selection': None,\n",
    "        'base_dir': 'training'\n",
    "}\n",
    "\n",
    "if first_half:\n",
    "    config['train_selection'] = 'train_selection_first_pulses.csv'\n",
    "    config['validate_selection'] = 'validate_selection_first_pulses.csv'\n",
    "else:\n",
    "    config['train_selection'] = 'train_selection_second_pulses.csv'\n",
    "    config['validate_selection'] = 'validate_selection_second_pulses.csv'\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train from scratch (slow) - remember to save it!\n",
    "#model = train_dynedge_from_scratch(config = config)\n",
    "\n",
    "# Load state-dict from pre-trained model (faster)\n",
    "model = load_pretrained_model(config, retrain_dict)\n",
    "model = train_dynedge_restart(model, config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if first_half:\n",
    "    torch.save(model.state_dict(), f'last_state_dict_{NAME}_first.pth')\n",
    "else:\n",
    "    torch.save(model.state_dict(), f'last_state_dict_{NAME}_second.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference & Evaluation\n",
    "\n",
    "With a trained model loaded into memory, we can now apply the model to batch_51. The following cells will start inference (or load in a csv with predictions, if you're in a hurry) and plot the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "del model\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "if first_half:\n",
    "    path_name =  f'last_state_dict_{NAME}_first.pth'\n",
    "else:\n",
    "    path_name =  f'last_state_dict_{NAME}_second.pth'\n",
    "\n",
    "model = load_pretrained_model(config = config, state_dict_path = path_name)\n",
    "config[\"batch_size\"] = 32\n",
    "results = inference(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_3d(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Converts zenith and azimuth to 3D direction vectors\"\"\"\n",
    "    df['true_x'] = np.cos(df['azimuth']) * np.sin(df['zenith'])\n",
    "    df['true_y'] = np.sin(df['azimuth'])*np.sin(df['zenith'])\n",
    "    df['true_z'] = np.cos(df['zenith'])\n",
    "    return df\n",
    "\n",
    "def calculate_angular_error(df : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Calcualtes the opening angle (angular error) between true and reconstructed direction vectors\"\"\"\n",
    "    df['angular_error'] = np.arccos(df['true_x']*df['direction_x'] + df['true_y']*df['direction_y'] + df['true_z']*df['direction_z'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = convert_to_3d(results)\n",
    "results = calculate_angular_error(results)\n",
    "results.to_csv(f'result_{first_half}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = results[\"angular_error\"].mean()\n",
    "print ('score=', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "fig = plt.figure(figsize = (6,6))\n",
    "plt.hist(results['angular_error'], \n",
    "         bins = np.arange(0,np.pi*2, 0.05), \n",
    "         histtype = 'step', \n",
    "         label = f'mean angular error: {np.round(results[\"angular_error\"].mean(),2)}')\n",
    "plt.xlabel('Angular Error [rad.]', size = 15)\n",
    "plt.ylabel('Counts', size = 15)\n",
    "plt.title('Angular Error Distribution (Batch 51)', size = 15)\n",
    "plt.legend(frameon = False, fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphnet_pure",
   "language": "python",
   "name": "graphnet_pure"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
