{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensor efficiency\n",
    "# https://www.kaggle.com/code/antonsevostianov/icecube-sensor-efficiency-feature-engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"SQLite-specific utility functions for use in `graphnet.data`.\"\"\"\n",
    "\n",
    "import os.path\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "def database_exists(database_path: str) -> bool:\n",
    "    \"\"\"Check whether database exists at `database_path`.\"\"\"\n",
    "    assert database_path.endswith(\n",
    "        \".db\"\n",
    "    ), \"Provided database path does not end in `.db`.\"\n",
    "    return os.path.exists(database_path)\n",
    "\n",
    "\n",
    "def database_table_exists(database_path: str, table_name: str) -> bool:\n",
    "    \"\"\"Check whether `table_name` exists in database at `database_path`.\"\"\"\n",
    "    if not database_exists(database_path):\n",
    "        return False\n",
    "    query = f\"SELECT name FROM sqlite_master WHERE type='table' AND name='{table_name}';\"\n",
    "    with sqlite3.connect(database_path) as conn:\n",
    "        result = pd.read_sql(query, conn)\n",
    "    return len(result) == 1\n",
    "\n",
    "\n",
    "def run_sql_code(database_path: str, code: str) -> None:\n",
    "    \"\"\"Execute SQLite code.\n",
    "    Args:\n",
    "        database_path: Path to databases\n",
    "        code: SQLite code\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(database_path)\n",
    "    c = conn.cursor()\n",
    "    c.executescript(code)\n",
    "    c.close()\n",
    "\n",
    "\n",
    "def save_to_sql(df: pd.DataFrame, table_name: str, database_path: str) -> None:\n",
    "    \"\"\"Save a dataframe `df` to a table `table_name` in SQLite `database`.\n",
    "    Table must exist already.\n",
    "    Args:\n",
    "        df: Dataframe with data to be stored in sqlite table\n",
    "        table_name: Name of table. Must exist already\n",
    "        database_path: Path to SQLite database\n",
    "    \"\"\"\n",
    "    engine = sqlalchemy.create_engine(\"sqlite:///\" + database_path)\n",
    "    df.to_sql(table_name, con=engine, index=False, if_exists=\"append\")\n",
    "    engine.dispose()\n",
    "\n",
    "\n",
    "def attach_index(\n",
    "    database_path: str, table_name: str, index_column: str = \"event_no\"\n",
    ") -> None:\n",
    "    \"\"\"Attach the table (i.e., event) index.\n",
    "    Important for query times!\n",
    "    \"\"\"\n",
    "    code = (\n",
    "        \"PRAGMA foreign_keys=off;\\n\"\n",
    "        \"BEGIN TRANSACTION;\\n\"\n",
    "        f\"CREATE INDEX {index_column}_{table_name} \"\n",
    "        f\"ON {table_name} ({index_column});\\n\"\n",
    "        \"COMMIT TRANSACTION;\\n\"\n",
    "        \"PRAGMA foreign_keys=on;\"\n",
    "    )\n",
    "    run_sql_code(database_path, code)\n",
    "\n",
    "\n",
    "def create_table(\n",
    "    columns: List[str],\n",
    "    table_name: str,\n",
    "    database_path: str,\n",
    "    *,\n",
    "    index_column: str = \"event_no\",\n",
    "    default_type: str = \"NOT NULL\",\n",
    "    integer_primary_key: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"Create a table.\n",
    "    Args:\n",
    "        columns: Column names to be created in table.\n",
    "        table_name: Name of the table.\n",
    "        database_path: Path to the database.\n",
    "        index_column: Name of the index column.\n",
    "        default_type: The type used for all non-index columns.\n",
    "        integer_primary_key: Whether or not to create the `index_column` with\n",
    "            the `INTEGER PRIMARY KEY` type. Such a column is required to have\n",
    "            unique, integer values for each row. This is appropriate when the\n",
    "            table has one row per event, e.g., event-level MC truth. It is not\n",
    "            appropriate for pulse map series, particle-level MC truth, and\n",
    "            other such data that is expected to have more that one row per\n",
    "            event (i.e., with the same index).\n",
    "    \"\"\"\n",
    "    # Prepare column names and types\n",
    "    query_columns = []\n",
    "    for column in columns:\n",
    "        type_ = default_type\n",
    "        if column == index_column:\n",
    "            if integer_primary_key:\n",
    "                type_ = \"INTEGER PRIMARY KEY NOT NULL\"\n",
    "            else:\n",
    "                type_ = \"NOT NULL\"\n",
    "\n",
    "        query_columns.append(f\"{column} {type_}\")\n",
    "    query_columns_string = \", \".join(query_columns)\n",
    "\n",
    "    # Run SQL code\n",
    "    code = (\n",
    "        \"PRAGMA foreign_keys=off;\\n\"\n",
    "        f\"CREATE TABLE {table_name} ({query_columns_string});\\n\"\n",
    "        \"PRAGMA foreign_keys=on;\"\n",
    "    )\n",
    "    run_sql_code(\n",
    "        database_path,\n",
    "        code,\n",
    "    )\n",
    "\n",
    "    # Attaching index to all non-truth-like tables (e.g., pulse maps).\n",
    "    if not integer_primary_key:\n",
    "        attach_index(database_path, table_name, index_column=index_column)\n",
    "\n",
    "\n",
    "def create_table_and_save_to_sql(\n",
    "    df: pd.DataFrame,\n",
    "    table_name: str,\n",
    "    database_path: str,\n",
    "    *,\n",
    "    index_column: str = \"event_no\",\n",
    "    default_type: str = \"NOT NULL\",\n",
    "    integer_primary_key: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"Create table if it doesn't exist and save dataframe to it.\"\"\"\n",
    "    if not database_table_exists(database_path, table_name):\n",
    "        create_table(\n",
    "            df.columns,\n",
    "            table_name,\n",
    "            database_path,\n",
    "            index_column=index_column,\n",
    "            default_type=default_type,\n",
    "            integer_primary_key=integer_primary_key,\n",
    "        )\n",
    "    save_to_sql(df, table_name=table_name, database_path=database_path)\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from typing import Any, Dict, List, Optional\n",
    "import numpy as np\n",
    "\n",
    "#from graphnet.data.sqlite.sqlite_utilities import create_table\n",
    "\n",
    "def load_input(meta_batch: pd.DataFrame, input_data_folder: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Will load the corresponding detector readings associated with the meta data batch.\n",
    "        \"\"\"\n",
    "        batch_id = pd.unique(meta_batch['batch_id'])\n",
    "\n",
    "        assert len(batch_id) == 1, \"contains multiple batch_ids. Did you set the batch_size correctly?\"\n",
    "        \n",
    "        detector_readings = pd.read_parquet(path = f'{input_data_folder}/batch_{batch_id[0]}.parquet')\n",
    "        sensor_positions = geometry_table.loc[detector_readings['sensor_id'], ['x', 'y', 'z'] + ['absorbtion', 'scattering']]\n",
    "        sensor_positions.index = detector_readings.index\n",
    "\n",
    "        for column in sensor_positions.columns:\n",
    "            if column not in detector_readings.columns:\n",
    "                detector_readings[column] = sensor_positions[column]\n",
    "\n",
    "        detector_readings['auxiliary'] = detector_readings['auxiliary'].replace({True: 1, False: 0})\n",
    "        return detector_readings.reset_index()\n",
    "\n",
    "def add_to_table(database_path: str,\n",
    "                      df: pd.DataFrame,\n",
    "                      table_name:  str,\n",
    "                      is_primary_key: bool,\n",
    "                      ) -> None:\n",
    "    \"\"\"Writes meta data to sqlite table. \n",
    "\n",
    "    Args:\n",
    "        database_path (str): the path to the database file.\n",
    "        df (pd.DataFrame): the dataframe that is being written to table.\n",
    "        table_name (str, optional): The name of the meta table. Defaults to 'meta_table'.\n",
    "        is_primary_key(bool): Must be True if each row of df corresponds to a unique event_id. Defaults to False.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        create_table(   columns=  df.columns,\n",
    "                        database_path = database_path, \n",
    "                        table_name = table_name,\n",
    "                        integer_primary_key= is_primary_key,\n",
    "                        index_column = 'event_id')\n",
    "    except sqlite3.OperationalError as e:\n",
    "        if 'already exists' in str(e):\n",
    "            pass\n",
    "        else:\n",
    "            raise e\n",
    "    engine = sqlalchemy.create_engine(\"sqlite:///\" + database_path)\n",
    "    df.to_sql(table_name, con=engine, index=False, if_exists=\"append\", chunksize = 200000)\n",
    "    engine.dispose()\n",
    "    return\n",
    "\n",
    "def convert_to_sqlite(meta_data_path: str,\n",
    "                      database_path: str,\n",
    "                      input_data_folder: str,\n",
    "                      batch_size: int = 200000,\n",
    "                      batch_ids: Optional[List[int]] = None,) -> None:\n",
    "    \"\"\"Converts a selection of the Competition's parquet files to a single sqlite database.\n",
    "\n",
    "    Args:\n",
    "        meta_data_path (str): Path to the meta data file.\n",
    "        batch_size (int): the number of rows extracted from meta data file at a time. Keep low for memory efficiency.\n",
    "        database_path (str): path to database. E.g. '/my_folder/data/my_new_database.db'\n",
    "        input_data_folder (str): folder containing the parquet input files.\n",
    "        batch_ids (List[int]): The batch_ids you want converted. Defaults to None (all batches will be converted)\n",
    "    \"\"\"\n",
    "    if batch_ids is None:\n",
    "        batch_ids = np.arange(1,661,1).to_list()\n",
    "    else:\n",
    "        assert isinstance(batch_ids,list), \"Variable 'batch_ids' must be list.\"\n",
    "    if not database_path.endswith('.db'):\n",
    "        database_path = database_path+'.db'\n",
    "    meta_data_iter = pq.ParquetFile(meta_data_path).iter_batches(batch_size = batch_size)\n",
    "    batch_id = 1\n",
    "    converted_batches = []\n",
    "    progress_bar = tqdm(total = len(batch_ids))\n",
    "    for meta_data_batch in meta_data_iter:\n",
    "        if batch_id in batch_ids:\n",
    "            meta_data_batch  = meta_data_batch.to_pandas()\n",
    "            add_to_table(database_path = database_path,\n",
    "                        df = meta_data_batch,\n",
    "                        table_name='meta_table',\n",
    "                        is_primary_key= True)\n",
    "            pulses = load_input(meta_batch=meta_data_batch, input_data_folder= input_data_folder)\n",
    "            del meta_data_batch # memory\n",
    "            add_to_table(database_path = database_path,\n",
    "                        df = pulses,\n",
    "                        table_name='pulse_table',\n",
    "                        is_primary_key= False)\n",
    "            del pulses # memory\n",
    "            progress_bar.update(1)\n",
    "            converted_batches.append(batch_id)\n",
    "        batch_id +=1\n",
    "        if len(batch_ids) == len(converted_batches):\n",
    "            break\n",
    "    progress_bar.close()\n",
    "    del meta_data_iter # memory\n",
    "    print(f'Conversion Complete!. Database available at\\n {database_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def ice_transparency(data_path, datum=1950):\n",
    "    # Data from page 31 of https://arxiv.org/pdf/1301.5361.pdf\n",
    "    # Datum is from footnote 8 of page 29\n",
    "    df = pd.read_csv(data_path, delim_whitespace=True)\n",
    "    df[\"z\"] = df[\"depth\"] - datum\n",
    "    df[\"z_norm\"] = df[\"z\"] / 500\n",
    "    df[[\"scattering_len_norm\", \"absorption_len_norm\"]] = RobustScaler().fit_transform(\n",
    "        df[[\"scattering_len\", \"absorption_len\"]]\n",
    "    )\n",
    "\n",
    "    # These are both roughly equivalent after scaling\n",
    "    f_scattering = interp1d(df[\"z_norm\"], df[\"scattering_len_norm\"])\n",
    "    f_absorption = interp1d(df[\"z_norm\"], df[\"absorption_len_norm\"])\n",
    "    return f_scattering, f_absorption\n",
    "\n",
    "f_scattering, f_absorption = ice_transparency('../../../input/ice-cube2023/dataset/ice_transparency/ice_transparency.csv')\n",
    "geometry_table = pd.read_csv('../../../input/icecube-neutrinos-in-deep-ice/sensor_geometry.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_absorption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry_table[\"absorbtion\"] = f_absorption(geometry_table[\"z\"] / 500)\n",
    "geometry_table[\"scattering\"] = f_scattering(geometry_table[\"z\"] / 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detector_group(x, z):\n",
    "    \"\"\"\n",
    "    Assigns values - deepcore, dustlayer, abovedust, underdust -\n",
    "    depending on sensor coordinates x and z\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define functions for each sensor category\n",
    "    def is_deepcore(x, z):\n",
    "        return x in {57.2, -9.68, 31.25, 72.37, 113.19, 106.94, 41.6, -10.97} or \\\n",
    "               (x in {46.29, 194.34, 90.49, -32.96, -77.8, 1.71, 124.97} and \n",
    "                ((z <= 186.02 and z >= 95.91) or \n",
    "                 (z <= -157 and z >= -511)))\n",
    "    \n",
    "    def is_dustlayer(z):\n",
    "        return z <= 0 and z >= -155\n",
    "    \n",
    "    def is_abovedust(x, z):\n",
    "        return z > 0 and not is_deepcore(x, z) and not is_dustlayer(z)\n",
    "    \n",
    "    def is_underdust(x, z):\n",
    "        return not is_deepcore(x, z) and not is_dustlayer(z) and not is_abovedust(x, z)\n",
    "    \n",
    "    # Check which sensor category the coordinates belong to\n",
    "    if is_deepcore(x, z):\n",
    "        return \"deepcore\"\n",
    "    \n",
    "    if is_dustlayer(z):\n",
    "        return \"dustlayer\"\n",
    "    \n",
    "    if is_abovedust(x, z):\n",
    "        return \"abovedust\"\n",
    "    \n",
    "    return \"underdust\"\n",
    "\n",
    "def relative_qe(group):\n",
    "    \"\"\"\n",
    "    Returns a relative quantum efficiency of a sensor group according to the following rules:\n",
    "    - 1.35, if sensor is deepcore;\n",
    "    - 0.95, if sensor is abovedust;\n",
    "    - 1.05 if sensor is underdust;\n",
    "    - 0.6 if sensor is dustlayer\n",
    "    \"\"\"\n",
    "    if group == 'deepcore':\n",
    "        return 1.35\n",
    "    if group == 'abovedust':\n",
    "        return 0.95\n",
    "    if group == 'underdust':\n",
    "        return 1.05\n",
    "    else:\n",
    "        return 0.6\n",
    "\n",
    "\n",
    "#geometry_table['sensor_group'] = geometry_table.apply(lambda row: detector_group(row['x'], row['z']), axis=1)\n",
    "#geometry_table['relative_qe'] = geometry_table['sensor_group'].apply(relative_qe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "enc = LabelEncoder()\n",
    "geometry_table['sensor_group']  = enc.fit_transform(geometry_table['sensor_group'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_data_folder = '../../../input/icecube-neutrinos-in-deep-ice/train'\n",
    "meta_data_path = '../../../input/icecube-neutrinos-in-deep-ice/train_meta.parquet'\n",
    "\n",
    "# database_path = f'/home/isamu/kaggledata/ICECUBE/batch_eff_1'\n",
    "# convert_to_sqlite(meta_data_path,\n",
    "#                   database_path=database_path,\n",
    "#                   input_data_folder=input_data_folder,\n",
    "#                   batch_ids = [1] )\n",
    "\n",
    "# database_path = f'/home/isamu/kaggledata/ICECUBE/batch_eff_51'\n",
    "# convert_to_sqlite(meta_data_path,\n",
    "#                   database_path=database_path,\n",
    "#                   input_data_folder=input_data_folder,\n",
    "#                   batch_ids = [51] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bids = list(range(1,100,1))\n",
    "bids.remove(51)\n",
    "\n",
    "database_path = f'/home/isamu/kaggledata/ICECUBE/batch_eff_1_100'\n",
    "convert_to_sqlite(meta_data_path,\n",
    "                  database_path=database_path,\n",
    "                  input_data_folder=input_data_folder,\n",
    "                  batch_ids = bids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bids = list(range(100,200,1))\n",
    "\n",
    "database_path = f'/home/isamu/kaggledata/ICECUBE/batch_eff_100_200'\n",
    "convert_to_sqlite(meta_data_path,\n",
    "                  database_path=database_path,\n",
    "                  input_data_folder=input_data_folder,\n",
    "                  batch_ids = bids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bids = list(range(200,300,1))\n",
    "\n",
    "database_path = f'/home/isamu/kaggledata/ICECUBE/batch_eff_200_300'\n",
    "convert_to_sqlite(meta_data_path,\n",
    "                  database_path=database_path,\n",
    "                  input_data_folder=input_data_folder,\n",
    "                  batch_ids = bids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
